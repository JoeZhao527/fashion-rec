{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your transactions data\n",
    "articles_df = pd.read_csv('./dataset/articles.csv')\n",
    "customers_df = pd.read_csv('./dataset/customers.csv')\n",
    "training_transactions = pd.read_csv('./dataset/split/fold_0/train.csv')\n",
    "validation_transactions = pd.read_csv('./dataset/split/fold_0/test.csv')\n",
    "# Load your transactions data\n",
    "\n",
    "print(\"articles_df: \", articles_df.columns)\n",
    "print(\"customers_df: \", customers_df.columns)\n",
    "print(\"transactions_df: \", training_transactions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transactions[\"article_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Positive Samples\n",
    "# Group by customer and list all articles they have purchased\n",
    "positive_samples = training_transactions.groupby('customer_id')['article_id'].agg(list).reset_index()\n",
    "\n",
    "# All unique articles\n",
    "all_articles = set(articles_df['article_id'].astype(str))\n",
    "\n",
    "# Step 3: Generate Negative Samples\n",
    "# Function to generate negative samples with a ratio of 1:5\n",
    "def generate_negatives(row):\n",
    "    num_positives = len(row['article_id'])\n",
    "    num_negatives = num_positives * 1  # Ratio of 1 positive to 5 negatives\n",
    "    positives = set(row['article_id'])\n",
    "    possible_negatives = list(all_articles - positives)\n",
    "    num_negatives = min(num_negatives, len(possible_negatives))  # Avoid trying to sample more items than available\n",
    "    return np.random.choice(possible_negatives, num_negatives, replace=False).tolist()\n",
    "\n",
    "# Apply function to generate negative samples\n",
    "# positive_samples['negative_samples'] = positive_samples.apply(generate_negatives, axis=1)\n",
    "\n",
    "# Step 4: Compile Training Data\n",
    "# Format data as sentences: each user has two lists, one for positives, one for negatives\n",
    "training_data = []\n",
    "for _, row in positive_samples.iterrows():\n",
    "    training_data.append(row['article_id'])  # Positive sentence\n",
    "    # training_data.append(row['negative_samples'])  # Negative sentence\n",
    "\n",
    "# Now training_data can be used for embedding training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique items in the filtered training transactions\n",
    "unique_items_count = training_transactions['article_id'].nunique()\n",
    "print(f\"Number of unique items in the training transactions: {unique_items_count}\")\n",
    "\n",
    "# Calculate the number of unique users in the filtered training transactions\n",
    "unique_users_count = training_transactions['customer_id'].nunique()\n",
    "print(f\"Number of unique users in the training transactions: {unique_users_count}\")\n",
    "\n",
    "# Calculate the number of unique users in the filtered training transactions\n",
    "unique_users_count = validation_transactions['article_id'].nunique()\n",
    "print(f\"Number of unique items in the validation transactions: {unique_users_count}\")\n",
    "\n",
    "# Calculate the number of unique users in the filtered training transactions\n",
    "unique_users_count = validation_transactions['customer_id'].nunique()\n",
    "print(f\"Number of unique users in the validation transactions: {unique_users_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "import gensim\n",
    "assert gensim.models.word2vec.FAST_VERSION > -1\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "for purchase in training_data:\n",
    "    random.shuffle(purchase)\n",
    "    \n",
    "print(len(training_data))\n",
    "\n",
    "# Calculate the length of each sublist\n",
    "lengths = [len(sublist) for sublist in training_data]\n",
    "\n",
    "# Count the frequency of each length\n",
    "length_distribution = Counter(lengths)\n",
    "\n",
    "# Print the length distribution\n",
    "print(\"Length distribution of sublists:\")\n",
    "for length, count in sorted(length_distribution.items())[:10]:\n",
    "    print(f\"Length {length}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = Word2Vec(sentences=training_data,  # pre-processed list of movie lists\n",
    "                 epochs=15,                # number of iterations over the corpus\n",
    "                 min_count=10,               # item need to appear more than 10 times\n",
    "                 vector_size=128,         # embedding vector size\n",
    "                 workers=8,               # number of threads to use for training\n",
    "                 sg=1,                    # using skip-gram algorithm\n",
    "                 hs=0,                    # hierarchical softmax not used, we use negative sampling instead\n",
    "                 negative=5,              # number of negative samples\n",
    "                 window=9999)               # context window size\n",
    "\n",
    "duration = time.time() - start\n",
    "print(\"Time passed: {:.2f} seconds\".format(duration))\n",
    "# To save the model for later use\n",
    "model.save('item2vec.model')\n",
    "model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model\n",
    "# model = Word2Vec.load('item2vec.model')\n",
    "\n",
    "# Now you can use the model\n",
    "# For example, to find vectors or perform operations like finding similar items\n",
    "# print(model.wv['some_item'])  # Replace 'some_item' with a valid item name from your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `transactions_df` is your transaction dataset with 'customer_id' and 'article_id'\n",
    "# and `model` is your trained Word2Vec model\n",
    "\n",
    "# Group transactions by user and collect all item IDs per user\n",
    "user_items = training_transactions.groupby('customer_id')['article_id'].apply(list)\n",
    "\n",
    "# Define a function to calculate the average vector of items per user\n",
    "def calculate_user_profile(item_ids, model):\n",
    "    vectors = []\n",
    "    for item_id in item_ids:\n",
    "        if item_id in model.wv.key_to_index:  # Check if the item is in the model's vocabulary\n",
    "            vectors.append(model.wv[item_id])\n",
    "    if vectors:  # If there are any vectors, calculate the average\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no items are in the vocabulary\n",
    "\n",
    "# Calculate user profiles\n",
    "user_profiles = user_items.apply(calculate_user_profile, model=model)\n",
    "\n",
    "# Now `user_profiles` contains the vector representation (profile) for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming user_profiles is already defined as per the previous instructions\n",
    "\n",
    "# Print the shape of the user_profiles DataFrame\n",
    "print(\"Shape of user_profiles:\", user_profiles.shape)\n",
    "\n",
    "# Check the data type of the first item to ensure consistency\n",
    "if len(user_profiles) > 0:\n",
    "    first_element = user_profiles.iloc[0]\n",
    "    print(\"Data type of user profiles:\", type(first_element))\n",
    "    print(\"Length of a single user profile vector:\", len(first_element))\n",
    "\n",
    "# Check for null values\n",
    "null_data = user_profiles.isnull().any()\n",
    "print(\"Any null user profiles?:\", null_data)\n",
    "\n",
    "# Print the first few user profiles to check\n",
    "print(\"Sample user profiles:\")\n",
    "print(user_profiles.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming `model` and `user_profiles` are already defined\n",
    "# Convert item vectors to a tensor and move to the device\n",
    "item_ids = list(model.wv.index_to_key)\n",
    "item_vectors = torch.tensor([model.wv[item] for item in item_ids], dtype=torch.float, device=device)\n",
    "item_vectors_norm = item_vectors / item_vectors.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Convert user profiles to a tensor\n",
    "user_ids = list(user_profiles.keys())\n",
    "user_vectors = torch.tensor(list(user_profiles.values), dtype=torch.float, device=device)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 4096\n",
    "\n",
    "# Function to process batches and get recommendations\n",
    "def process_batch(start_idx, end_idx):\n",
    "    # Slice the batch\n",
    "    batch_user_vectors = user_vectors[start_idx:end_idx]\n",
    "    batch_user_vectors_norm = batch_user_vectors / batch_user_vectors.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = torch.mm(batch_user_vectors_norm, item_vectors_norm.t())\n",
    "    \n",
    "    # Get top 12 recommendations for each user in the batch\n",
    "    top_indices = torch.topk(similarities, 100, dim=1).indices\n",
    "    \n",
    "    # Map indices to item IDs\n",
    "    return {user_ids[i]: [item_ids[idx] for idx in top_indices[row_index].cpu().tolist()]\n",
    "            for row_index, i in enumerate(range(start_idx, end_idx))}\n",
    "\n",
    "# Process all batches and collect recommendations\n",
    "user_recommendations = {}\n",
    "for start_idx in tqdm(range(0, len(user_vectors), batch_size)):\n",
    "    end_idx = min(start_idx + batch_size, len(user_vectors))\n",
    "    user_recommendations.update(process_batch(start_idx, end_idx))\n",
    "\n",
    "# # Print or use these recommendations\n",
    "# for user_id, recommendations in user_recommendations.items():\n",
    "#     print(f\"Recommendations for User {user_id}: {recommendations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare actual purchases data\n",
    "actual_purchases = validation_transactions.groupby('customer_id')['article_id'].agg(list)\n",
    "\n",
    "# Step 3: Compute MAP@K\n",
    "def compute_map_k(actual, predicted, k=12):\n",
    "    actual_set = set(actual)\n",
    "    predicted = predicted[:k]  # consider only the top k predictions\n",
    "    hits = [1 if item in actual_set else 0 for item in predicted]\n",
    "    cumulative_hits = np.cumsum(hits)\n",
    "    precision_at_k = [hits[i] * cumulative_hits[i] / (i + 1) for i in range(len(hits))]\n",
    "    if actual_set:\n",
    "        return sum(precision_at_k) / min(len(actual_set), k)\n",
    "    return 0\n",
    "\n",
    "# Collect actual and predicted lists\n",
    "user_ids = actual_purchases.index.intersection(user_recommendations.keys())\n",
    "actual_lists = [actual_purchases.loc[user_id] for user_id in user_ids]\n",
    "predicted_lists = [user_recommendations[user_id] for user_id in user_ids]\n",
    "\n",
    "# Calculate MAP@12\n",
    "map_scores = [compute_map_k(actual, predicted, 12) for actual, predicted in zip(actual_lists, predicted_lists)]\n",
    "mean_average_precision = np.mean(map_scores)\n",
    "\n",
    "print(f\"MAP@12: {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Your provided function\n",
    "def calculate_map_at_n(predictions, ground_truth, top_n=12):\n",
    "    def precision_at_k(k, predicted, actual):\n",
    "        if len(predicted) > k:\n",
    "            predicted = predicted[:k]\n",
    "        return len(set(predicted) & set(actual)) / k\n",
    "\n",
    "    def average_precision(predicted, actual):\n",
    "        if not actual:\n",
    "            return 0.0\n",
    "        ap = 0.0\n",
    "        relevant_items = 0\n",
    "        for k in range(1, min(len(predicted), top_n) + 1):\n",
    "            if predicted[k-1] in actual:\n",
    "                relevant_items += 1\n",
    "                ap += precision_at_k(k, predicted, actual) * 1\n",
    "        return ap / min(len(actual), top_n)\n",
    "    \n",
    "    return average_precision(predictions, ground_truth)\n",
    "\n",
    "# Prepare actual and predicted lists\n",
    "user_ids = actual_purchases.index.intersection(user_recommendations.keys())\n",
    "actual_lists = [actual_purchases.loc[user_id] for user_id in user_ids]\n",
    "predicted_lists = [user_recommendations[user_id] for user_id in user_ids]\n",
    "\n",
    "# Calculate MAP@12 for each user and compute the mean\n",
    "map_scores = [calculate_map_at_n(predicted, actual, top_n=12) for actual, predicted in zip(actual_lists, predicted_lists)]\n",
    "mean_average_precision = np.mean(map_scores)\n",
    "\n",
    "print(f\"MAP@12: {mean_average_precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Your provided function\n",
    "def calculate_map_at_n(predictions, ground_truth, top_n=12):\n",
    "    def precision_at_k(k, predicted, actual):\n",
    "        if len(predicted) > k:\n",
    "            predicted = predicted[:k]\n",
    "        return len(set(predicted) & set(actual)) / k\n",
    "\n",
    "    def average_precision(predicted, actual):\n",
    "        if not actual:\n",
    "            return 0.0\n",
    "        ap = 0.0\n",
    "        relevant_items = 0\n",
    "        for k in range(1, min(len(predicted), top_n) + 1):\n",
    "            if predicted[k-1] in actual:\n",
    "                relevant_items += 1\n",
    "                ap += precision_at_k(k, predicted, actual) * 1\n",
    "        return ap / min(len(actual), top_n)\n",
    "    \n",
    "    return average_precision(predictions, ground_truth)\n",
    "\n",
    "# Step 1: Prepare actual purchases data\n",
    "actual_purchases = validation_transactions.groupby('customer_id')['article_id'].agg(list)\n",
    "\n",
    "# Step 3: Compute MAP@K\n",
    "def compute_map_k(actual, predicted, k=12):\n",
    "    actual_set = set(actual)\n",
    "    predicted = predicted[:k]  # consider only the top k predictions\n",
    "    hits = [1 if item in actual_set else 0 for item in predicted]\n",
    "    cumulative_hits = np.cumsum(hits)\n",
    "    precision_at_k = [hits[i] * cumulative_hits[i] / (i + 1) for i in range(len(hits))]\n",
    "    if actual_set:\n",
    "        return sum(precision_at_k) / min(len(actual_set), k)\n",
    "    return 0\n",
    "\n",
    "# Prepare actual and predicted lists\n",
    "user_ids = actual_purchases.index.intersection(user_recommendations.keys())\n",
    "actual_lists = [actual_purchases.loc[user_id] for user_id in user_ids]\n",
    "predicted_lists = [user_recommendations[user_id] for user_id in user_ids]\n",
    "\n",
    "# Calculate MAP@12 for each user using both methods\n",
    "map_scores_method_1 = [calculate_map_at_n(predicted, actual, top_n=12) for actual, predicted in zip(actual_lists, predicted_lists)]\n",
    "map_scores_method_2 = [compute_map_k(actual, predicted, 12) for actual, predicted in zip(actual_lists, predicted_lists)]\n",
    "\n",
    "# Find and print differences\n",
    "for user_id, score1, score2 in zip(user_ids, map_scores_method_1, map_scores_method_2):\n",
    "    if score1 != score2:\n",
    "        print(f\"User ID: {user_id}, Method 1 MAP@12: {score1}, Method 2 MAP@12: {score2}\")\n",
    "\n",
    "mean_average_precision_1 = np.mean(map_scores_method_1)\n",
    "mean_average_precision_2 = np.mean(map_scores_method_2)\n",
    "\n",
    "print(f\"Mean MAP@12 Method 1: {mean_average_precision_1}\")\n",
    "print(f\"Mean MAP@12 Method 2: {mean_average_precision_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user ID you want to inspect\n",
    "user_id_to_inspect = 'b7332c96a15893a068fc0a3efdea80d905b8fe577650272bd7f5f23678afe5c9'\n",
    "\n",
    "# Get the actual purchases for the user\n",
    "actual_purchases_user = actual_purchases.loc[user_id_to_inspect]\n",
    "\n",
    "# Get the predicted recommendations for the user\n",
    "predicted_recommendations_user = user_recommendations[user_id_to_inspect][:12]\n",
    "\n",
    "# Print the actual purchases and predicted recommendations\n",
    "print(f\"Actual Purchases for User {user_id_to_inspect}: {actual_purchases_user}\")\n",
    "print(f\"Predicted Recommendations for User {user_id_to_inspect}: {predicted_recommendations_user}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function 1: calculate_map_at_n\n",
    "def calculate_map_at_n(predictions, ground_truth, top_n=12):\n",
    "    def precision_at_k(k, predicted, actual):\n",
    "        if len(predicted) > k:\n",
    "            predicted = predicted[:k]\n",
    "        return len(set(predicted) & set(actual)) / k\n",
    "\n",
    "    def average_precision(predicted, actual):\n",
    "        if not actual:\n",
    "            return 0.0\n",
    "        ap = 0.0\n",
    "        relevant_items = 0\n",
    "        for k in range(1, min(len(predicted), top_n) + 1):\n",
    "            if predicted[k-1] in actual:\n",
    "                relevant_items += 1\n",
    "                ap += precision_at_k(k, predicted, actual) * 1\n",
    "        return ap / min(len(actual), top_n)\n",
    "    \n",
    "    return average_precision(predictions, ground_truth)\n",
    "\n",
    "# Function 2: compute_map_k\n",
    "def compute_map_k(actual, predicted, k=12):\n",
    "    actual_set = set(actual)\n",
    "    predicted = predicted[:k]  # consider only the top k predictions\n",
    "    hits = [1 if item in actual_set else 0 for item in predicted]\n",
    "    cumulative_hits = np.cumsum(hits)\n",
    "    precision_at_k = [hits[i] * cumulative_hits[i] / (i + 1) for i in range(len(hits))]\n",
    "    if actual_set:\n",
    "        return sum(precision_at_k) / min(len(actual_set), k)\n",
    "    return 0\n",
    "\n",
    "# Test example\n",
    "actual_items = [648256007, 648256007]\n",
    "predicted_items = [648256007, 684588007, 504154020, 648256001, 626813006, 504154021, 504154019, 684588008, 684588001, 745829002, 684588002, 712425003]\n",
    "top_n = 3\n",
    "\n",
    "# Run both functions\n",
    "map_at_n_result = calculate_map_at_n(predicted_items, actual_items, top_n)\n",
    "map_k_result = compute_map_k(actual_items, predicted_items, top_n)\n",
    "\n",
    "print(f\"MAP@{top_n} using calculate_map_at_n: {map_at_n_result}\")\n",
    "print(f\"MAP@{top_n} using compute_map_k: {map_k_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `user_profiles` is a pandas DataFrame\n",
    "print(user_profiles.head())  # Displays the first few rows\n",
    "print(user_profiles.info())  # Provides a concise summary of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from annoy import AnnoyIndex\n",
    "# import numpy as np\n",
    "\n",
    "# # Assume user_profiles is a DataFrame with user profiles as vectors\n",
    "# f = 32  # Length of item vectors that we are indexing, here 32 dimensions\n",
    "# t = AnnoyIndex(f, 'angular')  # Use 'angular' for cosine distance\n",
    "\n",
    "# # Adding all user vectors to the Annoy index\n",
    "# for i, vector in enumerate(user_profiles):\n",
    "#     t.add_item(i, vector)\n",
    "\n",
    "# # Building the index - more trees give higher precision when querying\n",
    "# t.build(10)  # 10 trees\n",
    "\n",
    "# # Getting the 10,000 nearest neighbors for each user\n",
    "# user_nearest_neighbors = {}\n",
    "# for i in range(len(user_profiles)):\n",
    "#     # The second parameter is the number of neighbors you want\n",
    "#     # it includes the item itself, so ask for 10001\n",
    "#     nearest_neighbors = t.get_nns_by_item(i, 1001, include_distances=False)[1:]  # exclude the item itself\n",
    "#     user_nearest_neighbors[user_profiles.index[i]] = [user_profiles.index[nn] for nn in nearest_neighbors]\n",
    "\n",
    "# # Now you have a dictionary with user IDs as keys and lists of the 10,000 nearest neighbors' user IDs as values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Identify Last Week's Transactions\n",
    "current_date = training_transactions['t_dat'].max()\n",
    "last_week_start = current_date - timedelta(days=7)\n",
    "last_week_transactions = training_transactions[(training_transactions['t_dat'] > last_week_start) & (training_transactions['t_dat'] <= current_date)]\n",
    "\n",
    "# Step 2: Aggregate Purchases for the 10,000 Nearest Customers\n",
    "# Creating a DataFrame for easy lookup\n",
    "last_week_transactions['count'] = 1  # This will be used to sum up purchases\n",
    "\n",
    "def get_top_300_items(user_id, neighbors_dict, transactions):\n",
    "    # Get neighbor IDs for the user\n",
    "    neighbors = neighbors_dict[user_id]\n",
    "    # Filter transactions for these neighbors\n",
    "    neighbor_transactions = transactions[transactions['customer_id'].isin(neighbors)]\n",
    "    # Aggregate purchase counts by article\n",
    "    item_counts = neighbor_transactions.groupby('article_id').agg({'count': 'sum'}).reset_index()\n",
    "    # Sort by purchase count descending and select top 300\n",
    "    top_items = item_counts.sort_values(by='count', ascending=False).head(300)['article_id'].tolist()\n",
    "    return top_items\n",
    "\n",
    "# Assuming user_nearest_neighbors is already populated\n",
    "user_top_300_items = {user_id: get_top_300_items(user_id, user_nearest_neighbors, last_week_transactions) for user_id in user_nearest_neighbors.keys()}\n",
    "\n",
    "# Optionally, display or process these recommendations\n",
    "for user_id, top_items in user_top_300_items.items():\n",
    "    print(f\"User {user_id} top 300 recommended items: {top_items[:10]}\")  # Show only the top 10 for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare Actual Purchases Data\n",
    "# Assuming 'validation_transactions' has at least 'customer_id' and 'article_id'\n",
    "actual_purchases = validation_transactions.groupby('customer_id')['article_id'].agg(list)\n",
    "\n",
    "# Step 2: Extract Top 12 Predictions for Each User from Your Recommendations\n",
    "predicted_top_12 = {user_id: recommendations[:12] for user_id, recommendations in user_top_300_items.items()}\n",
    "\n",
    "# Step 3: Compute MAP@12\n",
    "def average_precision_at_k(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]  # truncate to the first k predictions\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def map_at_k(actual, predicted, k=12):\n",
    "    return np.mean([average_precision_at_k(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "# Collect actual and predicted lists for users present in both actual and predicted sets\n",
    "user_ids = set(actual_purchases.keys()).intersection(set(predicted_top_12.keys()))\n",
    "actual_lists = [actual_purchases[user_id] for user_id in user_ids]\n",
    "predicted_lists = [predicted_top_12[user_id] for user_id in user_ids]\n",
    "\n",
    "# Calculate MAP@12\n",
    "map_score = map_at_k(actual_lists, predicted_lists, 12)\n",
    "print(f\"MAP@12: {map_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Identify Most Popular Items in Training Data\n",
    "# Assuming `training_transactions` DataFrame has columns 'article_id'\n",
    "item_popularity = training_transactions['article_id'].value_counts().head(12)  # Get top 12 items\n",
    "most_popular_items = item_popularity.index.tolist()\n",
    "\n",
    "# Step 2: Recommend to Validation Users\n",
    "# Assuming `validation_transactions` DataFrame has column 'customer_id'\n",
    "validation_users = validation_transactions['customer_id'].unique()\n",
    "user_recommendations = {user: most_popular_items for user in validation_users}\n",
    "\n",
    "# Step 3: Calculate MAP@12\n",
    "# Step 1: Prepare actual purchases data\n",
    "actual_purchases = validation_transactions.groupby('customer_id')['article_id'].agg(list)\n",
    "\n",
    "def average_precision_at_k(actual, predicted, k=12):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def map_at_k(actual, predicted, k=12):\n",
    "    # Filter users who appear in both actual and predicted lists\n",
    "    common_users = set(actual.keys()).intersection(predicted.keys())\n",
    "    return np.mean([average_precision_at_k(actual[user], predicted[user], k) for user in common_users])\n",
    "\n",
    "# Calculate MAP@12\n",
    "map_score = map_at_k(actual_purchases, user_recommendations, 12)\n",
    "print(f\"MAP@12 for most popular item recommendations: {map_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert user_nearest_neighbors to a DataFrame\n",
    "neighbors_df = pd.DataFrame.from_dict(user_nearest_neighbors, orient='index')\n",
    "neighbors_df.reset_index(inplace=True)\n",
    "neighbors_df.columns = ['customer_id'] + [f'neighbor_{i+1}' for i in range(neighbors_df.shape[1]-1)]\n",
    "\n",
    "# Save to CSV\n",
    "neighbors_df.to_csv('user_nearest_neighbors.csv', index=False)\n",
    "print(\"Saved user_nearest_neighbors to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the top 10 most similar items\n",
    "item_id = \"851010002\"\n",
    "similar_items = model.wv.most_similar(item_id, topn=10)\n",
    "\n",
    "print(\"Top 10 similar items to {}:\".format(item_id))\n",
    "for item, similarity in similar_items:\n",
    "    print(\"Item: {}, Similarity: {:.4f}\".format(item, similarity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def get_image_path(item_id):\n",
    "    item_id = '0'+item_id\n",
    "    folder_number = item_id[:3]  # Assuming the first three digits of the item ID correspond to folder names\n",
    "    image_directory = r'./images/{}'.format(folder_number)\n",
    "    image_path = os.path.join(image_directory, '{}.jpg'.format(item_id))  # Assuming images are saved as PNG\n",
    "    print(image_path)\n",
    "    return image_path\n",
    "\n",
    "# Check if file exists and is file\n",
    "def is_valid_path(path):\n",
    "    return os.path.exists(path) and os.path.isfile(path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 2))  # Adjust the size as needed\n",
    "for i, (item, _) in enumerate(similar_items):\n",
    "    img_path = get_image_path(item)\n",
    "    if is_valid_path(img_path):\n",
    "        img = mpimg.imread(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(item)\n",
    "        axes[i].axis('off')  # Hide axes\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, 'No image', fontsize=12, ha='center')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
