{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles_df:  Index(['article_id', 'product_code', 'prod_name', 'product_type_no',\n",
      "       'product_type_name', 'product_group_name', 'graphical_appearance_no',\n",
      "       'graphical_appearance_name', 'colour_group_code', 'colour_group_name',\n",
      "       'perceived_colour_value_id', 'perceived_colour_value_name',\n",
      "       'perceived_colour_master_id', 'perceived_colour_master_name',\n",
      "       'department_no', 'department_name', 'index_code', 'index_name',\n",
      "       'index_group_no', 'index_group_name', 'section_no', 'section_name',\n",
      "       'garment_group_no', 'garment_group_name', 'detail_desc'],\n",
      "      dtype='object')\n",
      "customers_df:  Index(['customer_id', 'FN', 'Active', 'club_member_status',\n",
      "       'fashion_news_frequency', 'age', 'postal_code'],\n",
      "      dtype='object')\n",
      "transactions_df:  Index(['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your transactions data\n",
    "articles_df = pd.read_csv('./dataset/articles.csv')\n",
    "customers_df = pd.read_csv('./dataset/customers.csv')\n",
    "training_transactions = pd.read_csv('./dataset/split/fold_0/train.csv')\n",
    "validation_transactions = pd.read_csv('./dataset/split/fold_0/test.csv')\n",
    "# Load your transactions data\n",
    "\n",
    "print(\"articles_df: \", articles_df.columns)\n",
    "print(\"customers_df: \", customers_df.columns)\n",
    "print(\"transactions_df: \", training_transactions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "689109001    9297\n",
       "706016001    7728\n",
       "692930001    7558\n",
       "706016002    6422\n",
       "689109003    6187\n",
       "             ... \n",
       "636698001       1\n",
       "613347001       1\n",
       "664337001       1\n",
       "602152001       1\n",
       "739502002       1\n",
       "Name: count, Length: 42793, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_transactions[\"article_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Positive Samples\n",
    "# Group by customer and list all articles they have purchased\n",
    "positive_samples = training_transactions.groupby('customer_id')['article_id'].agg(list).reset_index()\n",
    "\n",
    "# All unique articles\n",
    "all_articles = set(articles_df['article_id'].astype(str))\n",
    "\n",
    "# Step 3: Generate Negative Samples\n",
    "# Function to generate negative samples with a ratio of 1:5\n",
    "def generate_negatives(row):\n",
    "    num_positives = len(row['article_id'])\n",
    "    num_negatives = num_positives * 1  # Ratio of 1 positive to 5 negatives\n",
    "    positives = set(row['article_id'])\n",
    "    possible_negatives = list(all_articles - positives)\n",
    "    num_negatives = min(num_negatives, len(possible_negatives))  # Avoid trying to sample more items than available\n",
    "    return np.random.choice(possible_negatives, num_negatives, replace=False).tolist()\n",
    "\n",
    "# Apply function to generate negative samples\n",
    "# positive_samples['negative_samples'] = positive_samples.apply(generate_negatives, axis=1)\n",
    "\n",
    "# Step 4: Compile Training Data\n",
    "# Format data as sentences: each user has two lists, one for positives, one for negatives\n",
    "training_data = []\n",
    "for _, row in positive_samples.iterrows():\n",
    "    training_data.append(row['article_id'])  # Positive sentence\n",
    "    # training_data.append(row['negative_samples'])  # Negative sentence\n",
    "\n",
    "# Now training_data can be used for embedding training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items in the training transactions: 42793\n",
      "Number of unique users in the training transactions: 493897\n",
      "Number of unique items in the validation transactions: 22190\n",
      "Number of unique users in the validation transactions: 85437\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique items in the filtered training transactions\n",
    "unique_items_count = training_transactions['article_id'].nunique()\n",
    "print(f\"Number of unique items in the training transactions: {unique_items_count}\")\n",
    "\n",
    "# Calculate the number of unique users in the filtered training transactions\n",
    "unique_users_count = training_transactions['customer_id'].nunique()\n",
    "print(f\"Number of unique users in the training transactions: {unique_users_count}\")\n",
    "\n",
    "# Calculate the number of unique users in the filtered training transactions\n",
    "unique_users_count = validation_transactions['article_id'].nunique()\n",
    "print(f\"Number of unique items in the validation transactions: {unique_users_count}\")\n",
    "\n",
    "# Calculate the number of unique users in the filtered training transactions\n",
    "unique_users_count = validation_transactions['customer_id'].nunique()\n",
    "print(f\"Number of unique users in the validation transactions: {unique_users_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493897\n",
      "Length distribution of sublists:\n",
      "Length 1: 70791 times\n",
      "Length 2: 71460 times\n",
      "Length 3: 56225 times\n",
      "Length 4: 47092 times\n",
      "Length 5: 36812 times\n",
      "Length 6: 30577 times\n",
      "Length 7: 24671 times\n",
      "Length 8: 20877 times\n",
      "Length 9: 17264 times\n",
      "Length 10: 14612 times\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "import gensim\n",
    "assert gensim.models.word2vec.FAST_VERSION > -1\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "for purchase in training_data:\n",
    "    random.shuffle(purchase)\n",
    "    \n",
    "print(len(training_data))\n",
    "\n",
    "# Calculate the length of each sublist\n",
    "lengths = [len(sublist) for sublist in training_data]\n",
    "\n",
    "# Count the frequency of each length\n",
    "length_distribution = Counter(lengths)\n",
    "\n",
    "# Print the length distribution\n",
    "print(\"Length distribution of sublists:\")\n",
    "for length, count in sorted(length_distribution.items())[:10]:\n",
    "    print(f\"Length {length}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 68.40 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = Word2Vec(sentences=training_data,  # pre-processed list of movie lists\n",
    "                 epochs=10,                # number of iterations over the corpus\n",
    "                 min_count=10,               # item need to appear more than 10 times\n",
    "                 vector_size=128,         # embedding vector size\n",
    "                 workers=8,               # number of threads to use for training\n",
    "                 sg=1,                    # using skip-gram algorithm\n",
    "                 hs=0,                    # hierarchical softmax not used, we use negative sampling instead\n",
    "                 negative=5,              # number of negative samples\n",
    "                 window=9999)               # context window size\n",
    "\n",
    "duration = time.time() - start\n",
    "print(\"Time passed: {:.2f} seconds\".format(duration))\n",
    "# To save the model for later use\n",
    "model.save('item2vec.model')\n",
    "model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load('item2vec.model')\n",
    "\n",
    "# Now you can use the model\n",
    "# For example, to find vectors or perform operations like finding similar items\n",
    "# print(model.wv['some_item'])  # Replace 'some_item' with a valid item name from your training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `transactions_df` is your transaction dataset with 'customer_id' and 'article_id'\n",
    "# and `model` is your trained Word2Vec model\n",
    "\n",
    "# Group transactions by user and collect all item IDs per user\n",
    "user_items = training_transactions.groupby('customer_id')['article_id'].apply(list)\n",
    "\n",
    "# Define a function to calculate the average vector of items per user\n",
    "def calculate_user_profile(item_ids, model):\n",
    "    vectors = []\n",
    "    for item_id in item_ids:\n",
    "        if item_id in model.wv.key_to_index:  # Check if the item is in the model's vocabulary\n",
    "            vectors.append(model.wv[item_id])\n",
    "    if vectors:  # If there are any vectors, calculate the average\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no items are in the vocabulary\n",
    "\n",
    "# Calculate user profiles\n",
    "user_profiles = user_items.apply(calculate_user_profile, model=model)\n",
    "\n",
    "# Now `user_profiles` contains the vector representation (profile) for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of user_profiles: (493897,)\n",
      "Data type of user profiles: <class 'numpy.ndarray'>\n",
      "Length of a single user profile vector: 128\n",
      "Any null user profiles?: False\n",
      "Sample user profiles:\n",
      "customer_id\n",
      "0000423b00ade91418cceaf3b26c6af3dd342b51fd051eec9c12fb36984420fa    [0.29293284, -0.30096897, 0.0062587056, 0.3340...\n",
      "000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318    [-0.2699006, 0.3842866, 0.017176077, -0.281941...\n",
      "00006413d8573cd20ed7128e53b7b13819fe5cfc2d801fe7fc0f26dd8d65a85a    [-0.3082838, 0.106087945, -0.44609258, -0.5450...\n",
      "00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2    [-0.11548444, 0.06403653, 0.022126459, -0.2546...\n",
      "00009d946eec3ea54add5ba56d5210ea898def4b46c68570cf0096d962cacc75    [-0.18852253, 0.048781775, 0.103449255, -0.143...\n",
      "Name: article_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assuming user_profiles is already defined as per the previous instructions\n",
    "\n",
    "# Print the shape of the user_profiles DataFrame\n",
    "print(\"Shape of user_profiles:\", user_profiles.shape)\n",
    "\n",
    "# Check the data type of the first item to ensure consistency\n",
    "if len(user_profiles) > 0:\n",
    "    first_element = user_profiles.iloc[0]\n",
    "    print(\"Data type of user profiles:\", type(first_element))\n",
    "    print(\"Length of a single user profile vector:\", len(first_element))\n",
    "\n",
    "# Check for null values\n",
    "null_data = user_profiles.isnull().any()\n",
    "print(\"Any null user profiles?:\", null_data)\n",
    "\n",
    "# Print the first few user profiles to check\n",
    "print(\"Sample user profiles:\")\n",
    "print(user_profiles.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20080\\3181531547.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  item_vectors = torch.tensor([model.wv[item] for item in item_ids], dtype=torch.float, device=device)\n",
      "100%|██████████| 121/121 [00:27<00:00,  4.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming `model` and `user_profiles` are already defined\n",
    "# Convert item vectors to a tensor and move to the device\n",
    "item_ids = list(model.wv.index_to_key)\n",
    "item_vectors = torch.tensor([model.wv[item] for item in item_ids], dtype=torch.float, device=device)\n",
    "item_vectors_norm = item_vectors / item_vectors.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Convert user profiles to a tensor\n",
    "user_ids = list(user_profiles.keys())\n",
    "user_vectors = torch.tensor(list(user_profiles.values), dtype=torch.float, device=device)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 4096\n",
    "\n",
    "# Function to process batches and get recommendations\n",
    "def process_batch(start_idx, end_idx):\n",
    "    # Slice the batch\n",
    "    batch_user_vectors = user_vectors[start_idx:end_idx]\n",
    "    batch_user_vectors_norm = batch_user_vectors / batch_user_vectors.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = torch.mm(batch_user_vectors_norm, item_vectors_norm.t())\n",
    "    \n",
    "    # Get top 12 recommendations for each user in the batch\n",
    "    top_indices = torch.topk(similarities, 100, dim=1).indices\n",
    "    \n",
    "    # Map indices to item IDs\n",
    "    return {user_ids[i]: [item_ids[idx] for idx in top_indices[row_index].cpu().tolist()]\n",
    "            for row_index, i in enumerate(range(start_idx, end_idx))}\n",
    "\n",
    "# Process all batches and collect recommendations\n",
    "user_recommendations = {}\n",
    "for start_idx in tqdm(range(0, len(user_vectors), batch_size)):\n",
    "    end_idx = min(start_idx + batch_size, len(user_vectors))\n",
    "    user_recommendations.update(process_batch(start_idx, end_idx))\n",
    "\n",
    "# # Print or use these recommendations\n",
    "# for user_id, recommendations in user_recommendations.items():\n",
    "#     print(f\"Recommendations for User {user_id}: {recommendations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@12: 0.02060477670642518\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare actual purchases data\n",
    "actual_purchases = validation_transactions.groupby('customer_id')['article_id'].agg(list)\n",
    "\n",
    "# Step 3: Compute MAP@K\n",
    "def compute_map_k(actual, predicted, k=12):\n",
    "    actual_set = set(actual)\n",
    "    predicted = predicted[:k]  # consider only the top k predictions\n",
    "    hits = [1 if item in actual_set else 0 for item in predicted]\n",
    "    cumulative_hits = np.cumsum(hits)\n",
    "    precision_at_k = [hits[i] * cumulative_hits[i] / (i + 1) for i in range(len(hits))]\n",
    "    if actual_set:\n",
    "        return sum(precision_at_k) / min(len(actual_set), k)\n",
    "    return 0\n",
    "\n",
    "# Collect actual and predicted lists\n",
    "user_ids = actual_purchases.index.intersection(user_recommendations.keys())\n",
    "actual_lists = [actual_purchases.loc[user_id] for user_id in user_ids]\n",
    "predicted_lists = [user_recommendations[user_id] for user_id in user_ids]\n",
    "\n",
    "# Calculate MAP@12\n",
    "map_scores = [compute_map_k(actual, predicted, 12) for actual, predicted in zip(actual_lists, predicted_lists)]\n",
    "mean_average_precision = np.mean(map_scores)\n",
    "\n",
    "print(f\"MAP@12: {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id\n",
      "0000423b00ade91418cceaf3b26c6af3dd342b51fd051eec9c12fb36984420fa    [0.29293284, -0.30096897, 0.0062587056, 0.3340...\n",
      "000058a12d5b43e67d225668fa1f8d618c13dc232df0cad8ffe7ad4a1091e318    [-0.2699006, 0.3842866, 0.017176077, -0.281941...\n",
      "00006413d8573cd20ed7128e53b7b13819fe5cfc2d801fe7fc0f26dd8d65a85a    [-0.3082838, 0.106087945, -0.44609258, -0.5450...\n",
      "00007d2de826758b65a93dd24ce629ed66842531df6699338c5570910a014cc2    [-0.11548444, 0.06403653, 0.022126459, -0.2546...\n",
      "00009d946eec3ea54add5ba56d5210ea898def4b46c68570cf0096d962cacc75    [-0.18852253, 0.048781775, 0.103449255, -0.143...\n",
      "Name: article_id, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 493897 entries, 0000423b00ade91418cceaf3b26c6af3dd342b51fd051eec9c12fb36984420fa to ffffcf35913a0bee60e8741cb2b4e78b8a98ee5ff2e6a1778d0116cffd259264\n",
      "Series name: article_id\n",
      "Non-Null Count   Dtype \n",
      "--------------   ----- \n",
      "493897 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 7.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Assuming `user_profiles` is a pandas DataFrame\n",
    "print(user_profiles.head())  # Displays the first few rows\n",
    "print(user_profiles.info())  # Provides a concise summary of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of Nearest Neighbors:\n",
      " [[     0 207708 279399 ... 130743 114100 114725]\n",
      " [185146      1 320948 ... 350678 441397 112082]\n",
      " [     2  54962  69753 ... 329052 247147 160619]\n",
      " ...\n",
      " [493894 466129 418769 ... 405148 106578 436438]\n",
      " [493895 320099 377801 ... 481992 367136 278758]\n",
      " [493896 163935  77990 ...  25654 209144 310742]]\n",
      "Distances to Nearest Neighbors:\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 1.0000000e+00\n",
      "  1.0000000e+00 1.0000000e+00]\n",
      " [3.5762787e-07 3.5762787e-07 3.5762787e-07 ... 8.3011955e-01\n",
      "  8.3015001e-01 8.3027041e-01]\n",
      " [2.3841858e-07 2.3841858e-07 2.3841858e-07 ... 4.3050027e-01\n",
      "  4.3163669e-01 4.3184638e-01]\n",
      " ...\n",
      " [2.3841858e-07 2.3841858e-07 1.0207671e-01 ... 6.0582107e-01\n",
      "  6.0639137e-01 6.0759407e-01]\n",
      " [1.1920929e-07 2.9190326e-01 3.2647753e-01 ... 5.1127470e-01\n",
      "  5.1132840e-01 5.1170492e-01]\n",
      " [0.0000000e+00 3.0488384e-01 3.0615306e-01 ... 4.3315709e-01\n",
      "  4.3430209e-01 4.3452853e-01]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "\n",
    "# Assuming `user_profiles` is a pandas Series where index is user_id and value is the average embedding vector.\n",
    "# First, ensure that all vectors have the same length (this step might be redundant if you're sure about the uniformity).\n",
    "if all(len(x) == len(user_profiles.iloc[0]) for x in user_profiles):\n",
    "    # Convert Series of lists or arrays to a 2D NumPy array\n",
    "    user_profiles_array = np.stack(user_profiles.values).astype('float32')\n",
    "\n",
    "    # Normalize the vectors\n",
    "    user_profiles_array = normalize(user_profiles_array)\n",
    "\n",
    "    # Create the Faiss index (using L2 distance here)\n",
    "    index = faiss.IndexFlatL2(user_profiles_array.shape[1])\n",
    "\n",
    "    # Use GPU for Faiss (ensure you have initialized GPU resources)\n",
    "    gpu_resources = faiss.StandardGpuResources()  # Using standard GPU resources\n",
    "    gpu_index = faiss.index_cpu_to_gpu(gpu_resources, 0, index)  # Move index to GPU 0\n",
    "    gpu_index.add(user_profiles_array)  # Add vectors to the index\n",
    "\n",
    "    # Search for the nearest neighbors\n",
    "    k = 200  # Number of nearest neighbors\n",
    "    distances, indices = gpu_index.search(user_profiles_array, k)\n",
    "\n",
    "    print(\"Indices of Nearest Neighbors:\\n\", indices)\n",
    "    print(\"Distances to Nearest Neighbors:\\n\", distances)\n",
    "else:\n",
    "    print(\"Error: Not all vectors have the same length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Step 1: Filter for last week's transactions\n",
    "# Convert 't_dat' to datetime if it's not already\n",
    "training_transactions['t_dat'] = pd.to_datetime(training_transactions['t_dat'])\n",
    "\n",
    "# Now, compute the current date and last week's start date\n",
    "current_date = training_transactions['t_dat'].max()\n",
    "last_week_start = current_date - timedelta(days=14)\n",
    "last_week_transactions = training_transactions[(training_transactions['t_dat'] > last_week_start) & (training_transactions['t_dat'] <= current_date)]\n",
    "\n",
    "# Group transactions by customer_id and aggregate article counts\n",
    "grouped_transactions = last_week_transactions.groupby('customer_id')['article_id'].agg(lambda x: x.value_counts().to_dict()).to_dict()\n",
    "\n",
    "# Step 2: Create a mapping from indices to user IDs\n",
    "user_id_list = user_profiles.index.tolist()  # List of user IDs corresponding to indices in user_profiles_array\n",
    "neighbors_dict = {user_id: [user_id_list[idx] for idx in indices[i]] for i, user_id in enumerate(user_id_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 493897/493897 [00:58<00:00, 8472.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 3: Function to find top items\n",
    "def get_top_items_for_user(user_id, neighbors_dict, grouped_transactions, top_k=12):\n",
    "    # Dictionary to hold aggregated article counts from all similar users\n",
    "    article_counts = {}\n",
    "\n",
    "    # Aggregate counts from all similar users\n",
    "    for neighbor_id in neighbors_dict[user_id]:\n",
    "        if neighbor_id in grouped_transactions:\n",
    "            neighbor_articles = grouped_transactions[neighbor_id]\n",
    "            for article_id, count in neighbor_articles.items():\n",
    "                if article_id in article_counts:\n",
    "                    article_counts[article_id] += count\n",
    "                else:\n",
    "                    article_counts[article_id] = count\n",
    "\n",
    "    # Sort articles by their aggregated counts and select the top_k articles\n",
    "    top_items = sorted(article_counts.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "    return [item[0] for item in top_items]  # Return only the article IDs\n",
    "\n",
    "# Step 4: Generate recommendations for each user\n",
    "# Generate recommendations for each user with a progress bar\n",
    "recommendations = {user_id: get_top_items_for_user(user_id, neighbors_dict, grouped_transactions, 12) for user_id in tqdm(user_id_list, desc=\"Generating recommendations\")}\n",
    "\n",
    "# Display or process the recommendations\n",
    "# for user_id, recommended_items in recommendations.items():\n",
    "#     print(f\"User {user_id} recommended items: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@12: 0.022427705001835836\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare actual purchases data\n",
    "actual_purchases = validation_transactions.groupby('customer_id')['article_id'].agg(list)\n",
    "\n",
    "# Function to compute MAP@k\n",
    "def compute_map_k(actual, predicted, k=12):\n",
    "    actual_set = set(actual)\n",
    "    predicted = predicted[:k]  # consider only the top k predictions\n",
    "    hits = [1 if item in actual_set else 0 for item in predicted]\n",
    "    cumulative_hits = np.cumsum(hits)\n",
    "    precision_at_k = [cumulative_hits[i] / (i + 1) if hits[i] else 0 for i in range(len(hits))]\n",
    "    if actual_set:\n",
    "        return np.sum(precision_at_k) / min(len(actual_set), k)\n",
    "    return 0\n",
    "\n",
    "# Step 2: Ensure user_recommendations is defined somewhere in your workflow\n",
    "# This should be a dictionary mapping user IDs to lists of recommended article IDs\n",
    "\n",
    "# Step 3: Collect actual and predicted lists\n",
    "user_ids = actual_purchases.index.intersection(recommendations.keys())\n",
    "actual_lists = [actual_purchases.loc[user_id] for user_id in user_ids]\n",
    "predicted_lists = [recommendations[user_id] for user_id in user_ids]\n",
    "\n",
    "# Calculate MAP@12\n",
    "map_scores = [compute_map_k(actual, predicted, 12) for actual, predicted in zip(actual_lists, predicted_lists)]\n",
    "mean_average_precision = np.mean(map_scores)\n",
    "\n",
    "print(f\"MAP@12: {mean_average_precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@12: 0.02025199129545246\n"
     ]
    }
   ],
   "source": [
    "def calculate_apk(actual, predicted, k=12):\n",
    "    \"\"\"Calculate average precision at k for a single user.\"\"\"\n",
    "    if not actual:\n",
    "        return 0\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, p in enumerate(predicted[:k]):\n",
    "        if p in actual and p not in predicted[:i]:  # Ensure uniqueness in predictions considered for scoring\n",
    "            hits += 1\n",
    "            score += hits / (i + 1)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "user_ids = set(actual_purchases.keys()).intersection(recommendations.keys())\n",
    "ap_scores = []\n",
    "\n",
    "for user_id in user_ids:\n",
    "    actual_items = actual_purchases[user_id]\n",
    "    predicted_items = recommendations[user_id]\n",
    "    ap = calculate_apk(actual_items, predicted_items, k=12)\n",
    "    ap_scores.append(ap)\n",
    "\n",
    "# Calculate the mean of the average precision scores for all users\n",
    "mean_average_precision = np.mean(ap_scores)\n",
    "print(f\"MAP@12: {mean_average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m aa\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aa' is not defined"
     ]
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the top 10 most similar items\n",
    "item_id = \"851010002\"\n",
    "similar_items = model.wv.most_similar(item_id, topn=10)\n",
    "\n",
    "print(\"Top 10 similar items to {}:\".format(item_id))\n",
    "for item, similarity in similar_items:\n",
    "    print(\"Item: {}, Similarity: {:.4f}\".format(item, similarity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def get_image_path(item_id):\n",
    "    item_id = '0'+item_id\n",
    "    folder_number = item_id[:3]  # Assuming the first three digits of the item ID correspond to folder names\n",
    "    image_directory = r'./images/{}'.format(folder_number)\n",
    "    image_path = os.path.join(image_directory, '{}.jpg'.format(item_id))  # Assuming images are saved as PNG\n",
    "    print(image_path)\n",
    "    return image_path\n",
    "\n",
    "# Check if file exists and is file\n",
    "def is_valid_path(path):\n",
    "    return os.path.exists(path) and os.path.isfile(path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 2))  # Adjust the size as needed\n",
    "for i, (item, _) in enumerate(similar_items):\n",
    "    img_path = get_image_path(item)\n",
    "    if is_valid_path(img_path):\n",
    "        img = mpimg.imread(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(item)\n",
    "        axes[i].axis('off')  # Hide axes\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, 'No image', fontsize=12, ha='center')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
