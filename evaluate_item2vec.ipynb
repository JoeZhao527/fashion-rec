{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles_df:  Index(['article_id', 'product_code', 'prod_name', 'product_type_no',\n",
      "       'product_type_name', 'product_group_name', 'graphical_appearance_no',\n",
      "       'graphical_appearance_name', 'colour_group_code', 'colour_group_name',\n",
      "       'perceived_colour_value_id', 'perceived_colour_value_name',\n",
      "       'perceived_colour_master_id', 'perceived_colour_master_name',\n",
      "       'department_no', 'department_name', 'index_code', 'index_name',\n",
      "       'index_group_no', 'index_group_name', 'section_no', 'section_name',\n",
      "       'garment_group_no', 'garment_group_name', 'detail_desc'],\n",
      "      dtype='object')\n",
      "customers_df:  Index(['customer_id', 'FN', 'Active', 'club_member_status',\n",
      "       'fashion_news_frequency', 'age', 'postal_code'],\n",
      "      dtype='object')\n",
      "transactions_df:  Index(['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your transactions data\n",
    "articles_df = pd.read_csv('./dataset/articles.csv')\n",
    "customers_df = pd.read_csv('./dataset/customers.csv')\n",
    "training_transactions = pd.read_csv('./dataset/split/fold_0/train.csv')\n",
    "validation_transactions = pd.read_csv('./dataset/split/fold_0/test.csv')\n",
    "# Load your transactions data\n",
    "\n",
    "print(\"articles_df: \", articles_df.columns)\n",
    "print(\"customers_df: \", customers_df.columns)\n",
    "print(\"transactions_df: \", training_transactions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Step 2: Positive Samples\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Group by customer and list all articles they have purchased\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m positive_samples \u001b[39m=\u001b[39m training_transactions\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mcustomer_id\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39;49m\u001b[39marticle_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49magg(\u001b[39mlist\u001b[39;49m)\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m      5\u001b[0m \u001b[39m# All unique articles\u001b[39;00m\n\u001b[0;32m      6\u001b[0m all_articles \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(articles_df[\u001b[39m'\u001b[39m\u001b[39marticle_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\yo\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:294\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_agg_general(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    293\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_agg_general(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    295\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     \u001b[39m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     \u001b[39m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[39m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aggregate_named(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\yo\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:327\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: func(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    326\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj_with_exclusions\n\u001b[1;32m--> 327\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grouper\u001b[39m.\u001b[39;49magg_series(obj, f)\n\u001b[0;32m    328\u001b[0m res \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_constructor(result, name\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mname)\n\u001b[0;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\yo\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[1;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39m_values, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    858\u001b[0m     \u001b[39m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[0;32m    859\u001b[0m     \u001b[39m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[0;32m    862\u001b[0m     preserve_dtype \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_series_pure_python(obj, func)\n\u001b[0;32m    866\u001b[0m npvalues \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    867\u001b[0m \u001b[39mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\yo\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m    882\u001b[0m splitter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_splitter(obj, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    884\u001b[0m \u001b[39mfor\u001b[39;00m i, group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(splitter):\n\u001b[1;32m--> 885\u001b[0m     res \u001b[39m=\u001b[39m func(group)\n\u001b[0;32m    886\u001b[0m     res \u001b[39m=\u001b[39m extract_result(res)\n\u001b[0;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m initialized:\n\u001b[0;32m    889\u001b[0m         \u001b[39m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\yo\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:324\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    322\u001b[0m     alias \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39m_builtin_table_alias[func]\n\u001b[0;32m    323\u001b[0m     warn_alias_replacement(\u001b[39mself\u001b[39m, orig_func, alias)\n\u001b[1;32m--> 324\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: func(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    326\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj_with_exclusions\n\u001b[0;32m    327\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grouper\u001b[39m.\u001b[39magg_series(obj, f)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\envs\\yo\\lib\\site-packages\\pandas\\core\\base.py:862\u001b[0m, in \u001b[0;36mIndexOpsMixin.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values)\n\u001b[0;32m    861\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values\u001b[39m.\u001b[39;49mitem, \u001b[39mrange\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values\u001b[39m.\u001b[39;49msize))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Step 2: Positive Samples\n",
    "# # Group by customer and list all articles they have purchased\n",
    "# positive_samples = training_transactions.groupby('customer_id')['article_id'].agg(list).reset_index()\n",
    "\n",
    "# # All unique articles\n",
    "# all_articles = set(articles_df['article_id'].astype(str))\n",
    "\n",
    "# # Step 4: Compile Training Data\n",
    "# # Format data as sentences: each user has two lists, one for positives, one for negatives\n",
    "# training_data = []\n",
    "# for _, row in positive_samples.iterrows():\n",
    "#     training_data.append(row['article_id'])  # Positive sentence\n",
    "#     # training_data.append(row['negative_samples'])  # Negative sentence\n",
    "\n",
    "# # Now training_data can be used for embedding training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "# warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "# warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "# import gensim\n",
    "# assert gensim.models.word2vec.FAST_VERSION > -1\n",
    "# from collections import Counter\n",
    "# import random\n",
    "\n",
    "# for purchase in training_data:\n",
    "#     random.shuffle(purchase)\n",
    "    \n",
    "# print(len(training_data))\n",
    "\n",
    "# # Calculate the length of each sublist\n",
    "# lengths = [len(sublist) for sublist in training_data]\n",
    "\n",
    "# # Count the frequency of each length\n",
    "# length_distribution = Counter(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# def train_and_save_models(sentences, vector_sizes, epochs=10, min_count=10, workers=8, sg=1, hs=0, negative=5, window=9999, save_path='./model'):\n",
    "#     results = []\n",
    "    \n",
    "#     # Ensure the save directory exists\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "#         print(f\"Directory {save_path} created.\")\n",
    "    \n",
    "#     for vector_size in vector_sizes:\n",
    "#         start = time.time()\n",
    "        \n",
    "#         # Train the model\n",
    "#         model = Word2Vec(\n",
    "#             sentences=sentences,\n",
    "#             epochs=epochs,\n",
    "#             min_count=min_count,\n",
    "#             vector_size=vector_size,\n",
    "#             workers=workers,\n",
    "#             sg=sg,\n",
    "#             hs=hs,\n",
    "#             negative=negative,\n",
    "#             window=window\n",
    "#         )\n",
    "        \n",
    "#         # Calculate training duration\n",
    "#         duration = time.time() - start\n",
    "        \n",
    "#         # Save the model\n",
    "#         model_save_path = f\"{save_path}/item2vec_{vector_size}.model\"\n",
    "#         model.save(model_save_path)\n",
    "        \n",
    "#         # Record results\n",
    "#         training_loss = model.get_latest_training_loss()\n",
    "#         results.append((vector_size, duration, training_loss, model_save_path))\n",
    "        \n",
    "#         print(f\"Model with vector_size {vector_size} saved to {model_save_path}. Time passed: {duration:.2f} seconds.\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Example usage:\n",
    "# vector_sizes = range(96, 257, 32)  # From 96 to 256 with a step of 32\n",
    "# train_and_save_models(training_data, vector_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:22<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@12 - Similarity: 0.01960577880033997, Popularity: 0.021528205169237827\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "assert gensim.models.word2vec.FAST_VERSION > -1\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "from datetime import timedelta\n",
    "\n",
    "# Ensure 't_dat' is a datetime object\n",
    "training_transactions['t_dat'] = pd.to_datetime(training_transactions['t_dat'])\n",
    "\n",
    "# Compute the current date and last week's start date\n",
    "current_date = training_transactions['t_dat'].max()\n",
    "last_week_start = current_date - timedelta(days=14)\n",
    "recent_transactions = training_transactions[(training_transactions['t_dat'] > last_week_start)]\n",
    "\n",
    "# Define function to calculate user profiles\n",
    "def calculate_user_profile(item_ids, model):\n",
    "    vectors = []\n",
    "    for item_id in item_ids:\n",
    "        if item_id in model.wv.key_to_index:  # Check if the item is in the model's vocabulary\n",
    "            vectors.append(model.wv[item_id])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Compute MAP@K for recommendation\n",
    "def compute_map_k(actual, predicted, k=12):\n",
    "    actual_set = set(actual)\n",
    "    predicted = predicted[:k]\n",
    "    hits = [1 if item in actual_set else 0 for item in predicted]\n",
    "    cumulative_hits = np.cumsum(hits)\n",
    "    precision_at_k = [hits[i] * cumulative_hits[i] / (i + 1) for i in range(len(hits))]\n",
    "    return sum(precision_at_k) / min(len(actual_set), k) if actual_set else 0\n",
    "\n",
    "# Function to create and query a FAISS index for popularity recommendations\n",
    "def create_faiss_index(user_profiles):\n",
    "    user_profiles_array = np.stack(user_profiles.values).astype('float32')\n",
    "    user_profiles_array = normalize(user_profiles_array)\n",
    "    index = faiss.IndexFlatL2(user_profiles_array.shape[1])\n",
    "    gpu_resources = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(gpu_resources, 0, index)\n",
    "    gpu_index.add(user_profiles_array)\n",
    "    k = 200  # Number of nearest neighbors\n",
    "    distances, indices = gpu_index.search(user_profiles_array, k)\n",
    "    return distances, indices\n",
    "\n",
    "# Function to process batches and get similarity recommendations\n",
    "def process_batch(batch_user_vectors, item_vectors_norm, item_ids, top_k=100):\n",
    "    batch_user_vectors_norm = batch_user_vectors / batch_user_vectors.norm(dim=1, keepdim=True)\n",
    "    similarities = torch.mm(batch_user_vectors_norm, item_vectors_norm.t())\n",
    "    top_indices = torch.topk(similarities, top_k, dim=1).indices\n",
    "    return top_indices\n",
    "\n",
    "# Function to aggregate recommendations based on nearest neighbors using pre-filtered transactions\n",
    "def aggregate_recommendations(user_profiles, indices, recent_transactions, k=12):\n",
    "    # Group transactions by customer_id and aggregate article counts\n",
    "    grouped_transactions = recent_transactions.groupby('customer_id')['article_id'].agg(lambda x: x.value_counts().to_dict()).to_dict()\n",
    "    \n",
    "    user_id_list = user_profiles.index.tolist()\n",
    "    recommendations = {}\n",
    "    \n",
    "    # Use tqdm to show the progress bar\n",
    "    for i in tqdm(range(len(user_id_list)), desc=\"Aggregating Recommendations\"):\n",
    "        user_id = user_id_list[i]\n",
    "        article_counts = {}\n",
    "        for idx in indices[i]:\n",
    "            neighbor_id = user_id_list[idx]\n",
    "            if neighbor_id in grouped_transactions:\n",
    "                for article_id, count in grouped_transactions[neighbor_id].items():\n",
    "                    article_counts[article_id] = article_counts.get(article_id, 0) + count\n",
    "        \n",
    "        # Sort articles by their aggregated counts and select the top_k articles\n",
    "        sorted_items = sorted(article_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        recommendations[user_id] = [item[0] for item in sorted_items[:k]]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Updated main evaluation function integrating both methods\n",
    "def evaluate_model(model, transactions_df, validation_transactions, device='cpu', batch_size=4096):\n",
    "    user_items = transactions_df.groupby('customer_id')['article_id'].apply(list)\n",
    "    user_profiles = user_items.apply(lambda x: calculate_user_profile(x, model))\n",
    "    \n",
    "    # Prepare tensors for similarity-based recommendations\n",
    "    item_ids = list(model.wv.index_to_key)\n",
    "    item_vectors = torch.tensor([model.wv[item] for item in item_ids], dtype=torch.float).to(device)\n",
    "    item_vectors_norm = item_vectors / item_vectors.norm(dim=1, keepdim=True)\n",
    "    user_ids = list(user_profiles.index)\n",
    "    user_vectors = torch.tensor(list(user_profiles.values), dtype=torch.float).to(device)\n",
    "    \n",
    "    # Compute similarity recommendations\n",
    "    similarity_recommendations = {}\n",
    "    for start_idx in tqdm(range(0, len(user_vectors), batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, len(user_vectors))\n",
    "        batch_user_vectors = user_vectors[start_idx:end_idx]\n",
    "        top_indices = process_batch(batch_user_vectors, item_vectors_norm, item_ids)\n",
    "        for row_index, i in enumerate(range(start_idx, end_idx)):\n",
    "            user_id = user_ids[i]\n",
    "            recommendations = [item_ids[idx] for idx in top_indices[row_index].cpu().tolist()]\n",
    "            similarity_recommendations[user_id] = recommendations\n",
    "    \n",
    "    # Compute popularity recommendations using FAISS\n",
    "    distances, indices = create_faiss_index(user_profiles)\n",
    "    popularity_recommendations = aggregate_recommendations(user_profiles, indices, recent_transactions)\n",
    "\n",
    "    # Prepare actual purchases data for MAP calculation\n",
    "    actual_purchases = validation_transactions.groupby('customer_id')['article_id'].agg(list)\n",
    "    actual_lists = [actual_purchases.loc[user_id] for user_id in actual_purchases.index if user_id in popularity_recommendations]\n",
    "\n",
    "    # Calculate MAP@12 for both recommendation methods\n",
    "    predicted_similarity_lists = [similarity_recommendations[user_id] for user_id in actual_purchases.index if user_id in similarity_recommendations]\n",
    "    predicted_popularity_lists = [popularity_recommendations[user_id] for user_id in actual_purchases.index if user_id in popularity_recommendations]\n",
    "\n",
    "    similarity_map_scores = [compute_map_k(actual, predicted, 12) for actual, predicted in zip(actual_lists, predicted_similarity_lists)]\n",
    "    popularity_map_scores = [compute_map_k(actual, predicted, 12) for actual, predicted in zip(actual_lists, predicted_popularity_lists)]\n",
    "\n",
    "    mean_similarity_map = np.mean(similarity_map_scores)\n",
    "    mean_popularity_map = np.mean(popularity_map_scores)\n",
    "    \n",
    "    return mean_similarity_map, mean_popularity_map\n",
    "\n",
    "\n",
    "# Usage example\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Word2Vec.load('model\\item2vec_96.model')  # Load the model\n",
    "mean_similarity_map, mean_popularity_map = evaluate_model(model, training_transactions, validation_transactions, device)\n",
    "print(f\"MAP@12 - Similarity: {mean_similarity_map}, Popularity: {mean_popularity_map}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
